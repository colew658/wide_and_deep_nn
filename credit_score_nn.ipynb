{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e298ecb",
   "metadata": {},
   "source": [
    "# Wide and Deep Networks for Credit Score Classification\n",
    "\n",
    "By: Joe, Sellett, Haiyan Cai, and Cole Wagner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78dc8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.utils import FeatureSpace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88cc846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df = pd.read_csv(\"credit_score_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "281cd4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "payment_behaviour\n",
       "Low_spent_Small_value_payments      26503\n",
       "High_spent_Medium_value_payments    18431\n",
       "Low_spent_Medium_value_payments     14516\n",
       "High_spent_Large_value_payments     14438\n",
       "High_spent_Small_value_payments     11850\n",
       "Low_spent_Large_value_payments      10958\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_df[\"payment_behaviour\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcb7a1",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d72d85",
   "metadata": {},
   "source": [
    "### Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1271ccf",
   "metadata": {},
   "source": [
    "Before proceeding with the modeling phase of this project, we will remove the following variables: customer_id, name, ssn, and type_of_loan. The customer_id field is being excluded because we already have a more robust unique identifier, id, which will serve as our primary reference for credit score reports. Similarly, name and ssn offer no predictive value and are being dropped to maintain data privacy and reduce dimensionality. Each of these variables contains approximately 8,000–12,000 unique values, whereas id contains over 96,000. Lastly, type_of_loan is being excluded for its high number of categories (50+), which would introduce unnecessary complexity. Instead, we will rely on the credit_mix variable, which summarizes loan diversity in a more manageable form as a category with only 3 unique values (standard, good, and bad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3ca9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df = credit_df.drop(\n",
    "    columns=[\"customer_id\", \"name\", \"ssn\", \"type_of_loan\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e7d43",
   "metadata": {},
   "source": [
    "### Create Feature Space for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9f7d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_dataframe(\n",
    "    x_input: pd.DataFrame, y_input: pd.Series, batch_size: int\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Convert a pandas dataframe to a TensorFlow Dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_input : pd.DataFrame\n",
    "        The input pandas dataframe containing the features.\n",
    "    y_input : pd.Series\n",
    "        The input pandas series containing the labels.\n",
    "    batch_size : int\n",
    "        The number of rows per batch in the TensorFlow Dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.data.Dataset\n",
    "        A TensorFlow Dataset object created from the input dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    df_dict = {\n",
    "        key: value.to_numpy()[:, np.newaxis]\n",
    "        for key, value in x_input.items()\n",
    "    }\n",
    "\n",
    "    tf_ds = tf.data.Dataset.from_tensor_slices((dict(df_dict), y_input))\n",
    "    tf_ds = tf_ds.batch(batch_size)\n",
    "    return tf_ds.prefetch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "263b4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample schema based on the dataframe info\n",
    "categorical_features = [\n",
    "    \"month\",\n",
    "    \"occupation\",\n",
    "    \"credit_mix\",\n",
    "    \"payment_of_min_amount\",\n",
    "    \"payment_behaviour\",\n",
    "]\n",
    "numeric_features = [\n",
    "    \"age\",\n",
    "    \"annual_income\",\n",
    "    \"monthly_inhand_salary\",\n",
    "    \"credit_history_age\",\n",
    "    \"total_emi_per_month\",\n",
    "    \"num_bank_accounts\",\n",
    "    \"num_credit_card\",\n",
    "    \"interest_rate\",\n",
    "    \"num_of_loan\",\n",
    "    \"delay_from_due_date\",\n",
    "    \"num_of_delayed_payment\",\n",
    "    \"changed_credit_limit\",\n",
    "    \"num_credit_inquiries\",\n",
    "    \"outstanding_debt\",\n",
    "    \"credit_utilization_ratio\",\n",
    "    \"amount_invested_monthly\",\n",
    "    \"monthly_balance\",\n",
    "]\n",
    "\n",
    "# Define feature configs\n",
    "feature_space = FeatureSpace(\n",
    "    features={\n",
    "        **{\n",
    "            name: FeatureSpace.string_categorical(num_oov_indices=0)\n",
    "            for name in categorical_features\n",
    "        },\n",
    "        **{\n",
    "            name: FeatureSpace.float_normalized()\n",
    "            for name in numeric_features\n",
    "        },\n",
    "    },\n",
    "    crosses=[\n",
    "        FeatureSpace.cross(\n",
    "            feature_names=(\"occupation\", \"credit_mix\"),\n",
    "            crossing_dim=15 * 3,\n",
    "        ),\n",
    "        FeatureSpace.cross(\n",
    "            feature_names=(\"payment_of_min_amount\", \"payment_behaviour\"),\n",
    "            crossing_dim=6 * 2,\n",
    "        ),\n",
    "    ],\n",
    "    output_mode=\"concat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3cf56",
   "metadata": {},
   "source": [
    "### Cross-Product Feature Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab16a1c",
   "metadata": {},
   "source": [
    "First, we created a cross-product feature between `occupation` and `credit_mix`. This combination allows us to capture differences in credit behavior across various professional backgrounds. For example, a neurosurgeon with a bad credit mix may exhibit very different financial behavior compared to an unemployed individual with the same credit mix. While each variable on its own may offer limited insight, their combination provides a more nuanced understanding of how occupation and credit diversity interact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19276d6",
   "metadata": {},
   "source": [
    "Another cross-product feature we created combines `payment_of_min_amount` and `payment_behavior`. The `payment_of_min_amount` variable is a binary indicator showing whether an individual made only the minimum payment on their debt for that month. In contrast, `payment_behavior` provides a broader description of a person’s spending and repayment patterns, such as “low spent, high payments” or “high spent, medium payments.” Since these two variables are closely related, their combination may help the model better capture nuanced repayment behaviors and improve its ability to distinguish between risk profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9904dd8",
   "metadata": {},
   "source": [
    "### Performance Metric Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491f7c8",
   "metadata": {},
   "source": [
    "Given the nature of our project, it’s important to evaluate our model using multiple metrics rather than relying solely on accuracy. In credit risk classification, false predictions carry different levels of business risk. For example, if a high-risk individual is incorrectly classified as low-risk, the company may absorb the financial loss from a bad loan. This makes recall especially important, as it tells us how well the model identifies actual high-risk cases and helps minimize false negatives. At the same time, precision matters because it reflects how accurate our high-risk predictions are, which ensures we don’t wrongly classify low-risk individuals as high-risk. A high recall means we’re catching most of the truly risky borrowers, while a high precision score means we’re correctly labeling them. Since both metrics are critical and often trade off against each other, we focus on the F1 score, which represents the harmonic mean of precision and recall. The F1 score gives us a more balanced and realistic measure of performance, especially in a setting where both catching risky borrowers and avoiding false alarms are essential to the business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d29cf4",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7046f",
   "metadata": {},
   "source": [
    "We have chosen to use a standard 80/20 train-test split for dividing our dataset. Given the size of our data (approximately 100,000 observations) we believe this approach is justified and will provide a reliable estimate of model performance. If our dataset were significantly smaller (around 1,000 observations), we might opt for 10-fold cross-validation to obtain a more stable and generalized result. Additionally, the 80/20 split offers a clear advantage in terms of computational efficiency. While 10-fold cross-validation could yield a marginal improvement in performance estimates, it would come at a considerable computational cost that is unnecessary given the scale of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9788c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    credit_df.drop(columns=[\"credit_score\", \"id\"]),\n",
    "    credit_df[\"credit_score\"],\n",
    "    test_size=0.2,\n",
    "    random_state=7324,\n",
    "    stratify=credit_df[\"credit_score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8f42248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to TensorFlow Datasets\n",
    "train_ds = create_dataset_from_dataframe(x_train, y_train, batch_size=32)\n",
    "test_ds = create_dataset_from_dataframe(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3d502fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 16:23:51.462825: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Apply feature space to datasets\n",
    "train_ds_no_labels = train_ds.map(lambda x, _: x)\n",
    "feature_space.adapt(train_ds_no_labels)\n",
    "processed_train_ds = train_ds.map(\n",
    "    lambda x, y: (feature_space(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "processed_train_ds = processed_train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds_no_labels = test_ds.map(lambda x, _: x)\n",
    "processed_test_ds = test_ds.map(\n",
    "    lambda x, y: (feature_space(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "processed_test_ds = processed_test_ds.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02822e45",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf08a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Flatten\n",
    "\n",
    "\n",
    "def setup_embedding_from_categorical(feature_space, col_name):\n",
    "    # what the maximum integer value for this variable?\n",
    "    # which is the same as the number of categories\n",
    "    N = len(feature_space.preprocessors[col_name].get_vocabulary())\n",
    "\n",
    "    # get the output from the feature space, which is input to embedding\n",
    "    x = feature_space.preprocessors[col_name].output\n",
    "\n",
    "    # now use an embedding to deal with integers from feature space\n",
    "    x = Embedding(\n",
    "        input_dim=N,\n",
    "        output_dim=int(np.sqrt(N)),\n",
    "        input_length=1,\n",
    "        name=col_name + \"_embed\",\n",
    "    )(x)\n",
    "\n",
    "    return Flatten()(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4756d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_embedding_from_crossing(feature_space, col_name):\n",
    "    # what the maximum integer value for this variable?\n",
    "\n",
    "    # get the size of the feature\n",
    "    N = feature_space.crossers[col_name].num_bins\n",
    "    x = feature_space.crossers[col_name].output\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(\n",
    "        input_dim=N,\n",
    "        output_dim=int(np.sqrt(N)),\n",
    "        input_length=1,\n",
    "        name=col_name + \"_embed\",\n",
    "    )(x)\n",
    "\n",
    "    return Flatten()(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd99897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_embedding_from_encoding(encoded_features, col_name):\n",
    "    # what the maximum integer value for this variable?\n",
    "\n",
    "    # get the size of the feature\n",
    "    x = encoded_features[col_name]\n",
    "    N = x.shape[1]\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(\n",
    "        input_dim=N,\n",
    "        output_dim=int(np.sqrt(N)),\n",
    "        input_length=1,\n",
    "        name=col_name + \"_embed\",\n",
    "    )(x)\n",
    "\n",
    "    x = Flatten()(\n",
    "        x\n",
    "    )  # get rid of that pesky extra dimension (for time of embedding)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8418fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wide_branches(dict_inputs, crossed_columns):\n",
    "    encoded_features = feature_space.get_encoded_features()\n",
    "\n",
    "    # we need to create separate lists for each branch\n",
    "    crossed_outputs = []\n",
    "\n",
    "    # for each crossed variable, make an embedding\n",
    "    for col in feature_space.crossers:\n",
    "        x = setup_embedding_from_encoding(encoded_features, col)\n",
    "\n",
    "        # save these outputs in list to concatenate later\n",
    "        crossed_outputs.append(x)\n",
    "\n",
    "    # now concatenate the outputs and add a fully connected layer\n",
    "    return Concatenate(name=\"wide_concat\")(crossed_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1958f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_branches(encoded_features, hidden_units=[64, 32]):\n",
    "    # reset this input branch\n",
    "    all_deep_branch_outputs = []\n",
    "\n",
    "    # for each numeric variable, just add it in after embedding\n",
    "    for col in numeric_features:\n",
    "        x = encoded_features[col]\n",
    "        # x = tf.cast(x,float) # cast an integer as a float here\n",
    "        all_deep_branch_outputs.append(x)\n",
    "\n",
    "    # for each categorical variable\n",
    "    for col in categorical_features:\n",
    "        # get the output tensor from ebedding layer\n",
    "        x = setup_embedding_from_encoding(encoded_features, col)\n",
    "\n",
    "        # save these outputs in list to concatenate later\n",
    "        all_deep_branch_outputs.append(x)\n",
    "    deep_branch = Concatenate(name=\"embed_concat\")(all_deep_branch_outputs)\n",
    "    i = 0\n",
    "    for layers in hidden_units:\n",
    "        name = \"deep\" + i\n",
    "        deep_branch = Dense(unit=layers, activation=\"relu\", name=name)(\n",
    "            deep_branch\n",
    "        )\n",
    "\n",
    "    return deep_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4df6dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "def build_wide_deep_networks(crossed_columns, hidden_units=[64, 32]):\n",
    "    dict_inputs = feature_space.get_inputs()  # need to use unprocessed features here, to gain access to each output\n",
    "    encoded_features = (\n",
    "        feature_space.get_encoded_features()\n",
    "    )  # these features have been encoded\n",
    "\n",
    "    wide_branch = build_wide_branches(dict_inputs, crossed_columns)\n",
    "    deep_branch = build_deep_branches(encoded_features, hidden_units)\n",
    "\n",
    "    # merge the deep and wide branch\n",
    "    final_branch = Concatenate(name=\"concat_deep_wide\")(\n",
    "        [deep_branch, wide_branch]\n",
    "    )\n",
    "    final_branch = Dense(units=1, activation=\"sigmoid\", name=\"combined\")(\n",
    "        final_branch\n",
    "    )\n",
    "\n",
    "    # encoded features input, fast\n",
    "    training_model = keras.Model(\n",
    "        inputs=encoded_features, outputs=final_branch\n",
    "    )\n",
    "    training_model.compile(\n",
    "        optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    training_model.summary()\n",
    "\n",
    "    # non-encoded, perform redundant operations\n",
    "    inference_model = keras.Model(inputs=dict_inputs, outputs=final_branch)\n",
    "    inference_model.compile(\n",
    "        loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    plot_model(\n",
    "        training_model,\n",
    "        to_file=\"model.png\",\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        rankdir=\"LR\",\n",
    "        expand_nested=False,\n",
    "        dpi=96,\n",
    "    )\n",
    "    return training_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "728c84f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor shape=(None, 108), dtype=float32, sparse=False, ragged=False, name=keras_tensor_63>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_space.get_encoded_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c46fcb2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FeatureSpace' object has no attribute 'get_crossed_encoded_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m crossed_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccupation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcredit_mix\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayment_of_min_amount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayment_behaviour\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      5\u001b[0m hidden_units \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m training_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_wide_deep_networks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrossed_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_units\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m history \u001b[38;5;241m=\u001b[39m training_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     10\u001b[0m     processed_train_ds,\n\u001b[1;32m     11\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     12\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mprocessed_test_ds,\n\u001b[1;32m     13\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     14\u001b[0m )\n",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m, in \u001b[0;36mbuild_wide_deep_networks\u001b[0;34m(crossed_columns, hidden_units)\u001b[0m\n\u001b[1;32m      5\u001b[0m dict_inputs \u001b[38;5;241m=\u001b[39m feature_space\u001b[38;5;241m.\u001b[39mget_inputs()  \u001b[38;5;66;03m# need to use unprocessed features here, to gain access to each output\u001b[39;00m\n\u001b[1;32m      6\u001b[0m encoded_features \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m     feature_space\u001b[38;5;241m.\u001b[39mget_encoded_features()\n\u001b[1;32m      8\u001b[0m )  \u001b[38;5;66;03m# these features have been encoded\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m wide_branch \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_wide_branches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdict_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrossed_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m deep_branch \u001b[38;5;241m=\u001b[39m build_deep_branches(encoded_features, hidden_units)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# merge the deep and wide branch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m, in \u001b[0;36mbuild_wide_branches\u001b[0;34m(dict_inputs, crossed_columns)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_wide_branches\u001b[39m(dict_inputs, crossed_columns):\n\u001b[0;32m----> 2\u001b[0m     crossed_encoded_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_crossed_encoded_features\u001b[49m()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# we need to create separate lists for each branch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     crossed_outputs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FeatureSpace' object has no attribute 'get_crossed_encoded_features'"
     ]
    }
   ],
   "source": [
    "crossed_columns = [\n",
    "    (\"occupation\", \"credit_mix\"),\n",
    "    (\"payment_of_min_amount\", \"payment_behaviour\"),\n",
    "]\n",
    "hidden_units = [64, 32]\n",
    "training_model = build_wide_deep_networks(crossed_columns, hidden_units)\n",
    "\n",
    "\n",
    "history = training_model.fit(\n",
    "    processed_train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=processed_test_ds,\n",
    "    verbose=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
