{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e298ecb",
   "metadata": {},
   "source": [
    "# Wide and Deep Networks for Credit Score Classification\n",
    "\n",
    "By: Joe, Sellett, Haiyan Cai, and Cole Wagner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78dc8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import FeatureSpace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cc846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df = pd.read_csv(\"credit_score_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcb7a1",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d72d85",
   "metadata": {},
   "source": [
    "### Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1271ccf",
   "metadata": {},
   "source": [
    "Before proceeding with the modeling phase of this project, we will remove the following variables: customer_id, name, ssn, and type_of_loan. The customer_id field is being excluded because we already have a more robust unique identifier, id, which will serve as our primary reference for credit score reports. Similarly, name and ssn offer no predictive value and are being dropped to maintain data privacy and reduce dimensionality. Each of these variables contains approximately 8,000–12,000 unique values, whereas id contains over 96,000. Lastly, type_of_loan is being excluded for its high number of categories (50+), which would introduce unnecessary complexity. Instead, we will rely on the credit_mix variable, which summarizes loan diversity in a more manageable form as a category with only 3 unique values (standard, good, and bad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ca9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df = credit_df.drop(\n",
    "    columns=[\"customer_id\", \"name\", \"ssn\", \"type_of_loan\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e7d43",
   "metadata": {},
   "source": [
    "### Create Feature Space for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f7d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_dataframe(\n",
    "    x_input: pd.DataFrame, y_input: pd.Series, batch_size: int\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Convert a pandas dataframe to a TensorFlow Dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_input : pd.DataFrame\n",
    "        The input pandas dataframe containing the features.\n",
    "    y_input : pd.Series\n",
    "        The input pandas series containing the labels.\n",
    "    batch_size : int\n",
    "        The number of rows per batch in the TensorFlow Dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.data.Dataset\n",
    "        A TensorFlow Dataset object created from the input dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    df_dict = {\n",
    "        key: value.to_numpy()[:, np.newaxis]\n",
    "        for key, value in x_input.items()\n",
    "    }\n",
    "\n",
    "    tf_ds = tf.data.Dataset.from_tensor_slices((dict(df_dict), y_input))\n",
    "    tf_ds = tf_ds.batch(batch_size)\n",
    "    return tf_ds.prefetch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263b4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample schema based on the dataframe info\n",
    "categorical_features = [\n",
    "    \"month\",\n",
    "    \"occupation\",\n",
    "    \"credit_mix\",\n",
    "    \"payment_of_min_amount\",\n",
    "    \"payment_behaviour\",\n",
    "]\n",
    "numeric_features = [\n",
    "    \"age\",\n",
    "    \"annual_income\",\n",
    "    \"monthly_inhand_salary\",\n",
    "    \"credit_history_age\",\n",
    "    \"total_emi_per_month\",\n",
    "    \"num_bank_accounts\",\n",
    "    \"num_credit_card\",\n",
    "    \"interest_rate\",\n",
    "    \"num_of_loan\",\n",
    "    \"delay_from_due_date\",\n",
    "    \"num_of_delayed_payment\",\n",
    "    \"changed_credit_limit\",\n",
    "    \"num_credit_inquiries\",\n",
    "    \"outstanding_debt\",\n",
    "    \"credit_utilization_ratio\",\n",
    "    \"amount_invested_monthly\",\n",
    "    \"monthly_balance\",\n",
    "]\n",
    "\n",
    "# Define feature configs\n",
    "feature_space = FeatureSpace(\n",
    "    features={\n",
    "        **{\n",
    "            name: FeatureSpace.string_categorical(num_oov_indices=0)\n",
    "            for name in categorical_features\n",
    "        },\n",
    "        **{\n",
    "            name: FeatureSpace.float_normalized()\n",
    "            for name in numeric_features\n",
    "        },\n",
    "    },\n",
    "    crosses=[\n",
    "        (\"occupation\", \"credit_mix\"),\n",
    "        (\"payment_of_min_amount\", \"payment_behaviour\"),\n",
    "    ],\n",
    "    output_mode=\"concat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3cf56",
   "metadata": {},
   "source": [
    "### Cross-Product Feature Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab16a1c",
   "metadata": {},
   "source": [
    "First, we created a cross-product feature between `occupation` and `credit_mix`. This combination allows us to capture differences in credit behavior across various professional backgrounds. For example, a neurosurgeon with a bad credit mix may exhibit very different financial behavior compared to an unemployed individual with the same credit mix. While each variable on its own may offer limited insight, their combination provides a more nuanced understanding of how occupation and credit diversity interact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19276d6",
   "metadata": {},
   "source": [
    "Another cross-product feature we created combines `payment_of_min_amount` and `payment_behavior`. The `payment_of_min_amount` variable is a binary indicator showing whether an individual made only the minimum payment on their debt for that month. In contrast, `payment_behavior` provides a broader description of a person’s spending and repayment patterns, such as “low spent, high payments” or “high spent, medium payments.” Since these two variables are closely related, their combination may help the model better capture nuanced repayment behaviors and improve its ability to distinguish between risk profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9904dd8",
   "metadata": {},
   "source": [
    "### Performance Metric Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491f7c8",
   "metadata": {},
   "source": [
    "Given the nature of our project, it’s important to evaluate our model using multiple metrics rather than relying solely on accuracy. In credit risk classification, false predictions carry different levels of business risk. For example, if a high-risk individual is incorrectly classified as low-risk, the company may absorb the financial loss from a bad loan. This makes recall especially important, as it tells us how well the model identifies actual high-risk cases and helps minimize false negatives. At the same time, precision matters because it reflects how accurate our high-risk predictions are, which ensures we don’t wrongly classify low-risk individuals as high-risk. A high recall means we’re catching most of the truly risky borrowers, while a high precision score means we’re correctly labeling them. Since both metrics are critical and often trade off against each other, we focus on the F1 score, which represents the harmonic mean of precision and recall. The F1 score gives us a more balanced and realistic measure of performance, especially in a setting where both catching risky borrowers and avoiding false alarms are essential to the business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d29cf4",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7046f",
   "metadata": {},
   "source": [
    "We have chosen to use a standard 80/20 train-test split for dividing our dataset. Given the size of our data (approximately 100,000 observations) we believe this approach is justified and will provide a reliable estimate of model performance. If our dataset were significantly smaller (around 1,000 observations), we might opt for 10-fold cross-validation to obtain a more stable and generalized result. Additionally, the 80/20 split offers a clear advantage in terms of computational efficiency. While 10-fold cross-validation could yield a marginal improvement in performance estimates, it would come at a considerable computational cost that is unnecessary given the scale of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9788c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    credit_df.drop(columns=[\"credit_score\", \"id\"]),\n",
    "    credit_df[\"credit_score\"],\n",
    "    test_size=0.2,\n",
    "    random_state=7324,\n",
    "    stratify=credit_df[\"credit_score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f42248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to TensorFlow Datasets\n",
    "train_ds = create_dataset_from_dataframe(x_train, y_train, batch_size=32)\n",
    "test_ds = create_dataset_from_dataframe(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d502fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 15:19:44.979140: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-04-12 15:19:45.620501: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-04-12 15:19:46.942109: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-04-12 15:19:51.684273: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-04-12 15:20:02.647269: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-04-12 15:20:13.460720: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Apply feature space to datasets\n",
    "train_ds_no_labels = train_ds.map(lambda x, _: x)\n",
    "feature_space.adapt(train_ds_no_labels)\n",
    "processed_train_ds = train_ds.map(\n",
    "    lambda x, y: (feature_space(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "processed_train_ds = processed_train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds_no_labels = test_ds.map(lambda x, _: x)\n",
    "feature_space.adapt(test_ds_no_labels)\n",
    "processed_test_ds = test_ds.map(\n",
    "    lambda x, y: (feature_space(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "processed_test_ds = processed_test_ds.prefetch(tf.data.AUTOTUNE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
